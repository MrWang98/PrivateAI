{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:my_env]",
      "language": "python",
      "name": "conda-env-my_env-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "MNIST_PATE_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivyclare/PrivateAI/blob/master/MNIST_PATE_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZi32riIbuSu",
        "colab_type": "text"
      },
      "source": [
        "### PATE Analysis on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuz3kCT6buSv",
        "colab_type": "text"
      },
      "source": [
        "http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html\n",
        "Our PATE approach at providing differential privacy to machine learning is based on a simple intuition: if two different classifiers, trained on two different datasets with no training examples in common, agree on how to classify a new input example, then that decision does not reveal information about any single training example. The decision could have been made with or without any single training example, because both the model trained with that example and the model trained without that example reached the same conclusion.\n",
        "\n",
        "===================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWHMA7l8buSw",
        "colab_type": "text"
      },
      "source": [
        "In order to train MNIST in a differentially private manner, we need 2 main components; private datasets (teachers) and public unlabelled dataset (student). MNIST is divided into train and test data. Hence, we'll have to create the teacher and student datasets ourselves. \n",
        "\n",
        "We will follow the steps below, to create a privacy preserving MNIST deep learning model:\n",
        "\n",
        "- Create the teacher and student datasets\n",
        "    - The training data is divided into non-overlapping subsets\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edWQXHlyiHqN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "33ff94d2-991c-4951-d83a-b886c7a1ba1e"
      },
      "source": [
        "# install syft package to use Private Aggregation of Teacher Ensembles (PATE)\n",
        "!pip install syft"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syft in /usr/local/lib/python3.6/dist-packages (0.2.4)\n",
            "Requirement already satisfied: flask-socketio~=4.2.1 in /usr/local/lib/python3.6/dist-packages (from syft) (4.2.1)\n",
            "Requirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.2)\n",
            "Requirement already satisfied: phe~=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.0.0)\n",
            "Requirement already satisfied: requests~=2.22.0 in /usr/local/lib/python3.6/dist-packages (from syft) (2.22.0)\n",
            "Requirement already satisfied: tornado==4.5.3 in /usr/local/lib/python3.6/dist-packages (from syft) (4.5.3)\n",
            "Requirement already satisfied: torchvision~=0.5.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.0)\n",
            "Requirement already satisfied: numpy~=1.18.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.18.3)\n",
            "Requirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.1)\n",
            "Requirement already satisfied: lz4~=3.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (3.0.2)\n",
            "Requirement already satisfied: torch~=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: websockets~=8.1.0 in /usr/local/lib/python3.6/dist-packages (from syft) (8.1)\n",
            "Requirement already satisfied: syft-proto~=0.2.5.a1 in /usr/local/lib/python3.6/dist-packages (from syft) (0.2.5a1)\n",
            "Requirement already satisfied: tblib~=1.6.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.6.0)\n",
            "Requirement already satisfied: websocket-client~=0.57.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.57.0)\n",
            "Requirement already satisfied: Pillow~=6.2.2 in /usr/local/lib/python3.6/dist-packages (from syft) (6.2.2)\n",
            "Requirement already satisfied: python-socketio>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from flask-socketio~=4.2.1->syft) (4.5.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (2.11.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (7.1.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision~=0.5.0->syft) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.11.1 in /usr/local/lib/python3.6/dist-packages (from syft-proto~=0.2.5.a1->syft) (3.11.3)\n",
            "Requirement already satisfied: python-engineio>=3.9.0 in /usr/local/lib/python3.6/dist-packages (from python-socketio>=4.3.0->flask-socketio~=4.2.1->syft) (3.12.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask~=1.1.1->syft) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.1->syft-proto~=0.2.5.a1->syft) (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOSQpjXfb6VX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c43091e-e913-4a14-d3ab-7c9200c02b89"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x697LZTXbuSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import our libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import time, os\n",
        "import math\n",
        "from syft.frameworks.torch.dp import pate\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jwIG2YJbuS1",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Create Teacher and Student Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaoJa_X5buS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load MNIST dataset\n",
        "\n",
        "data_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,),(0.5,))\n",
        "                                     ])\n",
        "# train_data = datasets.MNIST(root=’data’, train=True, download=True, transform=transform)\n",
        "\n",
        "trainset = datasets.MNIST(root='data', train=True, transform=data_transforms, download=True)\n",
        "\n",
        "testset = datasets.MNIST(root='data', train=False, transform=data_transforms, download=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3RyJT85buS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "07c296a5-c41e-44ea-effa-245036b0b1c3"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "len(trainset), len(testset)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDVQt12ebuS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5d23d69-4a6e-40d8-f094-b6299b3b5a57"
      },
      "source": [
        "# TEACHERS\n",
        "#divide train set between teachers and create dataloaders for valid and trainsets\n",
        "num_teachers = 10\n",
        "valid_per = 0.3 #20% for validation\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def teacher_dataloaders(transet=trainset, num_teachers=num_teachers, batch_size=batch_size, valid_per = 0.3):\n",
        "  trainloaders = []\n",
        "  validloaders = []\n",
        "  teacher_data_len = len(trainset) // num_teachers\n",
        "\n",
        "  for i in range(num_teachers):\n",
        "    # get particular subset of data\n",
        "    indice = list(range(i*teacher_data_len, (i+1)*teacher_data_len))\n",
        "    data_subset = Subset(trainset, indice)\n",
        "    # split into train and validation set\n",
        "    valid_size = int(len(data_subset) * valid_per)\n",
        "    train_size = len(data_subset) - valid_size\n",
        "    train_subset, valid_subset = torch.utils.data.random_split(data_subset, [train_size,valid_size])\n",
        "    # print(len(train_subset))\n",
        "    # print(len(valid_subset))\n",
        "\n",
        "    #create data loaders\n",
        "    trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "    validloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "    #add dataloaders to list\n",
        "    trainloaders.append(trainloader)\n",
        "    validloaders.append(validloader)\n",
        "  \n",
        "  return trainloaders, validloaders\n",
        "\n",
        "trainloaders, validloaders = teacher_dataloaders()\n",
        "len(trainloaders), len(validloaders)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnR9a5hdbuTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "77068845-9a07-423b-b32c-a553e4b2fa58"
      },
      "source": [
        "#  # STUDENT \n",
        "# split into train and validation set\n",
        "valid_size = int(len(testset) * 0.3)\n",
        "train_size = len(testset) - valid_size\n",
        "student_train_subset, student_valid_subset = torch.utils.data.random_split(testset, [train_size,valid_size])\n",
        "# print(len(train_subset))\n",
        "# print(len(valid_subset))\n",
        "\n",
        "#create data loaders\n",
        "student_train_loader = DataLoader(student_train_subset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "student_valid_loader = DataLoader(student_valid_subset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "len(student_train_loader), len(student_valid_loader)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(110, 47)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wahwYWtlobpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# split = 0.7\n",
        "# split_ind = int(len(testset) * split)\n",
        "  \n",
        "# # validset and testset\n",
        "# # validset is smaller because dataset of each teacher is small\n",
        "# private_ind = list(range(0, split_ind))\n",
        "# public_ind = list(range(split_ind, len(testset)))\n",
        "# private_set = Subset(testset, private_ind)\n",
        "# public_set = Subset(testset, public_ind)\n",
        "\n",
        "# # validloader and testloader\n",
        "# student_trainloader = torch.utils.data.DataLoader(private_set, batch_size=batch_size, shuffle=True)\n",
        "# student_validloader = torch.utils.data.DataLoader(public_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# len(student_trainloader), len(student_validloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgU7Sn3gjf1Z",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Train Teachers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3GrSyoFj40n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(784, 256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, 64)\n",
        "    self.fc4 = nn.Linear(64, 10)\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.dropout(F.relu(self.fc2(x)))\n",
        "    x = self.dropout(F.relu(self.fc3(x)))\n",
        "    x = F.log_softmax(self.fc4(x), dim=1)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFOJuHT_buTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training loop\n",
        "def train(trainloader, validloader, model, optimizer, criterion, epochs, device):\n",
        "  start = time.time()\n",
        "  trainloader = trainloaders[0]\n",
        "  validloader = validloaders[0]\n",
        "  best_loss = math.inf\n",
        "  train_results = []\n",
        "  valid_results = []\n",
        "\n",
        "  for epoch in range(epochs):   \n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    valid_corrects = 0\n",
        "    valid_loss = 0\n",
        "    \n",
        "    for images, labels in trainloader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(images)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      #running_loss += loss.item()\n",
        "      running_loss += loss.item() * images.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        model.eval()\n",
        "        for images, labels in validloader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model(images)\n",
        "          v_loss = criterion(outputs, labels)\n",
        "\n",
        "          valid_loss += loss.item() * images.size(0)\n",
        "          ps = torch.exp(outputs)\n",
        "          top_p, top_class = ps.topk(1, dim=1)\n",
        "          equals = top_class == labels.view(*top_class.shape)\n",
        "          valid_corrects += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "      #   # if(valid_loss < best_loss):\n",
        "      #   #   best_loss = valid_loss\n",
        "      \n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        train_acc = running_corrects.double() / len(trainloader)\n",
        "        train_results.append([train_loss,train_acc])\n",
        "\n",
        "        valid_losss = valid_loss / len(validloader)\n",
        "        valid_acc = valid_corrects / len(validloader)\n",
        "        valid_results.append([valid_losss,valid_acc])\n",
        "\n",
        "    print(\"Epoch: {}/{}\".format(epoch, epochs))\n",
        "    print('\\tTrain Loss: {:.4f} Train Acc: {:.4f}'.format(train_loss, train_acc))\n",
        "    print('\\tValid Loss: {:.4f} Valid Acc: {:.4f}'.format(valid_losss, valid_acc))\n",
        "  return model\n",
        "  # return model, train_results, valid_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA6uZB7_m4qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "#model.to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters() , lr=0.001)\n",
        "epochs = 10\n",
        "\n",
        "#train(trainloaders, validloaders, model, optimizer, criterion, epochs, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdYsYzDaSpIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# method for training\n",
        "def train(model, criterion, optimizer, trainloader, validloader, epochs=10):\n",
        "  model = model\n",
        "  criterion = criterion\n",
        "  optimizer = optimizer\n",
        "  epochs = epochs\n",
        "\n",
        "  train_losses, valid_losses = [], []\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for inputs, labels in trainloader:\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      log_ps = model(inputs)\n",
        "      loss = criterion(log_ps, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "  \n",
        "    else:\n",
        "      valid_loss = 0\n",
        "      acc = 0\n",
        "\n",
        "      # turn off gradients for validation, saving memory and computations\n",
        "      with torch.no_grad():\n",
        "        model.eval()\n",
        "        for inputs, labels in validloader:\n",
        "\n",
        "          log_ps = model(inputs)\n",
        "          valid_loss += criterion(log_ps, labels)\n",
        "\n",
        "          ps = torch.exp(log_ps)\n",
        "          top_p, top_class = ps.topk(1, dim=1)\n",
        "          equals = top_class == labels.view(*top_class.shape)\n",
        "          acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "      model.train()\n",
        "      train_losses.append(running_loss / len(trainloader))\n",
        "      valid_losses.append(valid_loss / len(validloader))\n",
        "\n",
        "      print('Epoch: {}/{}.. '.format(e+1, epochs),\n",
        "            'Training Loss: {:.3f}.. '.format(running_loss / len(trainloader)),\n",
        "            'Valid Loss: {:.3f}.. '.format(valid_loss / len(validloader)),\n",
        "            'Valid Accuracy: {:.3f} '.format(acc / len(validloader)),\n",
        "            '')\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkKBJOKFx4oS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d794e30a-d2f2-48ff-ab8b-169ebacfc53a"
      },
      "source": [
        "teacher_models = []\n",
        "i = 1\n",
        "for trainloader, validloader in zip(trainloaders, validloaders):\n",
        "  print(\" Training Teacher {}\".format(i))\n",
        "  #teacher_model = train(trainloaders, validloaders, model, optimizer, criterion, epochs, device)\n",
        "  teacher_model = train(model, criterion, optimizer, trainloader, validloader)\n",
        "  teacher_models.append(teacher_model)\n",
        "  i+=1\n",
        "  print(\"======================================================\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Training Teacher 1\n",
            "Epoch: 1/10..  Training Loss: 1.625..  Valid Loss: 0.774..  Valid Accuracy: 0.747  \n",
            "Epoch: 2/10..  Training Loss: 0.748..  Valid Loss: 0.476..  Valid Accuracy: 0.861  \n",
            "Epoch: 3/10..  Training Loss: 0.537..  Valid Loss: 0.405..  Valid Accuracy: 0.876  \n",
            "Epoch: 4/10..  Training Loss: 0.472..  Valid Loss: 0.362..  Valid Accuracy: 0.888  \n",
            "Epoch: 5/10..  Training Loss: 0.418..  Valid Loss: 0.320..  Valid Accuracy: 0.903  \n",
            "Epoch: 6/10..  Training Loss: 0.379..  Valid Loss: 0.363..  Valid Accuracy: 0.895  \n",
            "Epoch: 7/10..  Training Loss: 0.372..  Valid Loss: 0.288..  Valid Accuracy: 0.917  \n",
            "Epoch: 8/10..  Training Loss: 0.306..  Valid Loss: 0.289..  Valid Accuracy: 0.915  \n",
            "Epoch: 9/10..  Training Loss: 0.322..  Valid Loss: 0.291..  Valid Accuracy: 0.920  \n",
            "Epoch: 10/10..  Training Loss: 0.304..  Valid Loss: 0.332..  Valid Accuracy: 0.908  \n",
            "======================================================\n",
            " Training Teacher 2\n",
            "Epoch: 1/10..  Training Loss: 0.427..  Valid Loss: 0.318..  Valid Accuracy: 0.900  \n",
            "Epoch: 2/10..  Training Loss: 0.389..  Valid Loss: 0.277..  Valid Accuracy: 0.921  \n",
            "Epoch: 3/10..  Training Loss: 0.347..  Valid Loss: 0.236..  Valid Accuracy: 0.932  \n",
            "Epoch: 4/10..  Training Loss: 0.319..  Valid Loss: 0.249..  Valid Accuracy: 0.927  \n",
            "Epoch: 5/10..  Training Loss: 0.306..  Valid Loss: 0.261..  Valid Accuracy: 0.924  \n",
            "Epoch: 6/10..  Training Loss: 0.307..  Valid Loss: 0.247..  Valid Accuracy: 0.924  \n",
            "Epoch: 7/10..  Training Loss: 0.298..  Valid Loss: 0.267..  Valid Accuracy: 0.921  \n",
            "Epoch: 8/10..  Training Loss: 0.276..  Valid Loss: 0.292..  Valid Accuracy: 0.908  \n",
            "Epoch: 9/10..  Training Loss: 0.254..  Valid Loss: 0.247..  Valid Accuracy: 0.927  \n",
            "Epoch: 10/10..  Training Loss: 0.234..  Valid Loss: 0.239..  Valid Accuracy: 0.930  \n",
            "======================================================\n",
            " Training Teacher 3\n",
            "Epoch: 1/10..  Training Loss: 0.413..  Valid Loss: 0.341..  Valid Accuracy: 0.900  \n",
            "Epoch: 2/10..  Training Loss: 0.346..  Valid Loss: 0.265..  Valid Accuracy: 0.922  \n",
            "Epoch: 3/10..  Training Loss: 0.322..  Valid Loss: 0.276..  Valid Accuracy: 0.914  \n",
            "Epoch: 4/10..  Training Loss: 0.302..  Valid Loss: 0.266..  Valid Accuracy: 0.922  \n",
            "Epoch: 5/10..  Training Loss: 0.290..  Valid Loss: 0.258..  Valid Accuracy: 0.925  \n",
            "Epoch: 6/10..  Training Loss: 0.260..  Valid Loss: 0.248..  Valid Accuracy: 0.929  \n",
            "Epoch: 7/10..  Training Loss: 0.252..  Valid Loss: 0.242..  Valid Accuracy: 0.927  \n",
            "Epoch: 8/10..  Training Loss: 0.244..  Valid Loss: 0.240..  Valid Accuracy: 0.930  \n",
            "Epoch: 9/10..  Training Loss: 0.255..  Valid Loss: 0.269..  Valid Accuracy: 0.918  \n",
            "Epoch: 10/10..  Training Loss: 0.239..  Valid Loss: 0.262..  Valid Accuracy: 0.922  \n",
            "======================================================\n",
            " Training Teacher 4\n",
            "Epoch: 1/10..  Training Loss: 0.330..  Valid Loss: 0.231..  Valid Accuracy: 0.924  \n",
            "Epoch: 2/10..  Training Loss: 0.282..  Valid Loss: 0.201..  Valid Accuracy: 0.938  \n",
            "Epoch: 3/10..  Training Loss: 0.251..  Valid Loss: 0.238..  Valid Accuracy: 0.936  \n",
            "Epoch: 4/10..  Training Loss: 0.235..  Valid Loss: 0.199..  Valid Accuracy: 0.949  \n",
            "Epoch: 5/10..  Training Loss: 0.215..  Valid Loss: 0.192..  Valid Accuracy: 0.950  \n",
            "Epoch: 6/10..  Training Loss: 0.219..  Valid Loss: 0.242..  Valid Accuracy: 0.930  \n",
            "Epoch: 7/10..  Training Loss: 0.228..  Valid Loss: 0.198..  Valid Accuracy: 0.946  \n",
            "Epoch: 8/10..  Training Loss: 0.203..  Valid Loss: 0.219..  Valid Accuracy: 0.946  \n",
            "Epoch: 9/10..  Training Loss: 0.183..  Valid Loss: 0.206..  Valid Accuracy: 0.940  \n",
            "Epoch: 10/10..  Training Loss: 0.156..  Valid Loss: 0.198..  Valid Accuracy: 0.949  \n",
            "======================================================\n",
            " Training Teacher 5\n",
            "Epoch: 1/10..  Training Loss: 0.371..  Valid Loss: 0.202..  Valid Accuracy: 0.939  \n",
            "Epoch: 2/10..  Training Loss: 0.278..  Valid Loss: 0.207..  Valid Accuracy: 0.936  \n",
            "Epoch: 3/10..  Training Loss: 0.266..  Valid Loss: 0.174..  Valid Accuracy: 0.947  \n",
            "Epoch: 4/10..  Training Loss: 0.233..  Valid Loss: 0.253..  Valid Accuracy: 0.930  \n",
            "Epoch: 5/10..  Training Loss: 0.252..  Valid Loss: 0.209..  Valid Accuracy: 0.943  \n",
            "Epoch: 6/10..  Training Loss: 0.218..  Valid Loss: 0.201..  Valid Accuracy: 0.944  \n",
            "Epoch: 7/10..  Training Loss: 0.201..  Valid Loss: 0.189..  Valid Accuracy: 0.943  \n",
            "Epoch: 8/10..  Training Loss: 0.209..  Valid Loss: 0.246..  Valid Accuracy: 0.922  \n",
            "Epoch: 9/10..  Training Loss: 0.203..  Valid Loss: 0.200..  Valid Accuracy: 0.947  \n",
            "Epoch: 10/10..  Training Loss: 0.167..  Valid Loss: 0.192..  Valid Accuracy: 0.951  \n",
            "======================================================\n",
            " Training Teacher 6\n",
            "Epoch: 1/10..  Training Loss: 0.327..  Valid Loss: 0.210..  Valid Accuracy: 0.940  \n",
            "Epoch: 2/10..  Training Loss: 0.241..  Valid Loss: 0.240..  Valid Accuracy: 0.934  \n",
            "Epoch: 3/10..  Training Loss: 0.236..  Valid Loss: 0.223..  Valid Accuracy: 0.928  \n",
            "Epoch: 4/10..  Training Loss: 0.243..  Valid Loss: 0.226..  Valid Accuracy: 0.925  \n",
            "Epoch: 5/10..  Training Loss: 0.208..  Valid Loss: 0.186..  Valid Accuracy: 0.946  \n",
            "Epoch: 6/10..  Training Loss: 0.196..  Valid Loss: 0.226..  Valid Accuracy: 0.936  \n",
            "Epoch: 7/10..  Training Loss: 0.189..  Valid Loss: 0.220..  Valid Accuracy: 0.934  \n",
            "Epoch: 8/10..  Training Loss: 0.172..  Valid Loss: 0.192..  Valid Accuracy: 0.945  \n",
            "Epoch: 9/10..  Training Loss: 0.174..  Valid Loss: 0.190..  Valid Accuracy: 0.945  \n",
            "Epoch: 10/10..  Training Loss: 0.159..  Valid Loss: 0.210..  Valid Accuracy: 0.945  \n",
            "======================================================\n",
            " Training Teacher 7\n",
            "Epoch: 1/10..  Training Loss: 0.375..  Valid Loss: 0.307..  Valid Accuracy: 0.902  \n",
            "Epoch: 2/10..  Training Loss: 0.297..  Valid Loss: 0.205..  Valid Accuracy: 0.945  \n",
            "Epoch: 3/10..  Training Loss: 0.239..  Valid Loss: 0.189..  Valid Accuracy: 0.948  \n",
            "Epoch: 4/10..  Training Loss: 0.221..  Valid Loss: 0.196..  Valid Accuracy: 0.943  \n",
            "Epoch: 5/10..  Training Loss: 0.190..  Valid Loss: 0.218..  Valid Accuracy: 0.943  \n",
            "Epoch: 6/10..  Training Loss: 0.196..  Valid Loss: 0.211..  Valid Accuracy: 0.933  \n",
            "Epoch: 7/10..  Training Loss: 0.194..  Valid Loss: 0.207..  Valid Accuracy: 0.938  \n",
            "Epoch: 8/10..  Training Loss: 0.166..  Valid Loss: 0.211..  Valid Accuracy: 0.940  \n",
            "Epoch: 9/10..  Training Loss: 0.190..  Valid Loss: 0.217..  Valid Accuracy: 0.941  \n",
            "Epoch: 10/10..  Training Loss: 0.188..  Valid Loss: 0.239..  Valid Accuracy: 0.932  \n",
            "======================================================\n",
            " Training Teacher 8\n",
            "Epoch: 1/10..  Training Loss: 0.330..  Valid Loss: 0.218..  Valid Accuracy: 0.936  \n",
            "Epoch: 2/10..  Training Loss: 0.257..  Valid Loss: 0.220..  Valid Accuracy: 0.942  \n",
            "Epoch: 3/10..  Training Loss: 0.243..  Valid Loss: 0.192..  Valid Accuracy: 0.947  \n",
            "Epoch: 4/10..  Training Loss: 0.223..  Valid Loss: 0.197..  Valid Accuracy: 0.945  \n",
            "Epoch: 5/10..  Training Loss: 0.210..  Valid Loss: 0.190..  Valid Accuracy: 0.944  \n",
            "Epoch: 6/10..  Training Loss: 0.168..  Valid Loss: 0.184..  Valid Accuracy: 0.951  \n",
            "Epoch: 7/10..  Training Loss: 0.175..  Valid Loss: 0.209..  Valid Accuracy: 0.937  \n",
            "Epoch: 8/10..  Training Loss: 0.197..  Valid Loss: 0.208..  Valid Accuracy: 0.941  \n",
            "Epoch: 9/10..  Training Loss: 0.199..  Valid Loss: 0.186..  Valid Accuracy: 0.950  \n",
            "Epoch: 10/10..  Training Loss: 0.144..  Valid Loss: 0.173..  Valid Accuracy: 0.954  \n",
            "======================================================\n",
            " Training Teacher 9\n",
            "Epoch: 1/10..  Training Loss: 0.316..  Valid Loss: 0.209..  Valid Accuracy: 0.939  \n",
            "Epoch: 2/10..  Training Loss: 0.247..  Valid Loss: 0.197..  Valid Accuracy: 0.946  \n",
            "Epoch: 3/10..  Training Loss: 0.247..  Valid Loss: 0.194..  Valid Accuracy: 0.942  \n",
            "Epoch: 4/10..  Training Loss: 0.220..  Valid Loss: 0.165..  Valid Accuracy: 0.954  \n",
            "Epoch: 5/10..  Training Loss: 0.202..  Valid Loss: 0.200..  Valid Accuracy: 0.941  \n",
            "Epoch: 6/10..  Training Loss: 0.183..  Valid Loss: 0.191..  Valid Accuracy: 0.953  \n",
            "Epoch: 7/10..  Training Loss: 0.158..  Valid Loss: 0.203..  Valid Accuracy: 0.946  \n",
            "Epoch: 8/10..  Training Loss: 0.176..  Valid Loss: 0.183..  Valid Accuracy: 0.950  \n",
            "Epoch: 9/10..  Training Loss: 0.145..  Valid Loss: 0.212..  Valid Accuracy: 0.950  \n",
            "Epoch: 10/10..  Training Loss: 0.149..  Valid Loss: 0.177..  Valid Accuracy: 0.957  \n",
            "======================================================\n",
            " Training Teacher 10\n",
            "Epoch: 1/10..  Training Loss: 0.226..  Valid Loss: 0.119..  Valid Accuracy: 0.966  \n",
            "Epoch: 2/10..  Training Loss: 0.239..  Valid Loss: 0.141..  Valid Accuracy: 0.962  \n",
            "Epoch: 3/10..  Training Loss: 0.177..  Valid Loss: 0.160..  Valid Accuracy: 0.959  \n",
            "Epoch: 4/10..  Training Loss: 0.157..  Valid Loss: 0.113..  Valid Accuracy: 0.965  \n",
            "Epoch: 5/10..  Training Loss: 0.140..  Valid Loss: 0.132..  Valid Accuracy: 0.964  \n",
            "Epoch: 6/10..  Training Loss: 0.160..  Valid Loss: 0.132..  Valid Accuracy: 0.964  \n",
            "Epoch: 7/10..  Training Loss: 0.149..  Valid Loss: 0.159..  Valid Accuracy: 0.949  \n",
            "Epoch: 8/10..  Training Loss: 0.138..  Valid Loss: 0.131..  Valid Accuracy: 0.962  \n",
            "Epoch: 9/10..  Training Loss: 0.125..  Valid Loss: 0.134..  Valid Accuracy: 0.963  \n",
            "Epoch: 10/10..  Training Loss: 0.119..  Valid Loss: 0.159..  Valid Accuracy: 0.955  \n",
            "======================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNCxZEsMkAQ2",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Get Private Student Labels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx5ZjFHkkFWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "634c1c0c-08e0-46ab-8344-2bf2d9408c07"
      },
      "source": [
        "# get private labels\n",
        "def student_train_labels(teacher_models, dataloader):\n",
        "  student_labels = []\n",
        "  # get label for each teacher\n",
        "  for model in teacher_models:\n",
        "    student_label = []\n",
        "    for images,_ in dataloader:\n",
        "      with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(torch.exp(outputs), dim=1)\n",
        "      student_label.append(preds.tolist())\n",
        "    # add all teacher predictions to student_labels  \n",
        "    student_label = sum(student_label, [])\n",
        "    student_labels.append(student_label)\n",
        "  return student_labels\n",
        "\n",
        "# # label testset by models\n",
        "# def get_label_testset(models, testloader):\n",
        "#   test_labels = []\n",
        "#   for model in models:\n",
        "#     test_label = []\n",
        "#     for inputs, _ in testloader:\n",
        "#       with torch.no_grad():\n",
        "#         log_ps = model(inputs)\n",
        "#         ps = torch.exp(log_ps)\n",
        "#       top_p, top_class = ps.topk(1, dim=1)\n",
        "#       test_label.append(top_class.squeeze().tolist())\n",
        "#     test_label = sum(test_label, [])\n",
        "#     test_labels.append(test_label)\n",
        "#   return test_labels\n",
        "\n",
        "predicted_labels = student_train_labels(teacher_models, student_train_loader)     \n",
        "predicted_labels = np.array([np.array(p) for p in predicted_labels]).transpose(1, 0)\n",
        "# We see here that we have 10 labels for each of image in our dataset\n",
        "print(predicted_labels.shape)\n",
        "print(predicted_labels[2])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7000, 10)\n",
            "[7 7 7 7 7 7 7 7 7 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBQKOh94Umeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# labels = predicted_labels[1]\n",
        "# counts = np.bincount(labels, minlength=10)\n",
        "# query_result = np.argmax(counts)\n",
        "# query_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F99Sr27kGHu",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Add Laplacian Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-WP1kABkJ0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get private labels with the most votes count and add noise them\n",
        "def add_noise(predicted_labels, epsilon=0.1):\n",
        "  noisy_labels = []\n",
        "  for preds in predicted_labels:\n",
        "    # print(preds.shape[0])\n",
        "    # get labels with max votes\n",
        "    counts = np.bincount(preds, minlength=preds.shape[0])\n",
        "    # add laplacian noise to label\n",
        "    epsilon = epsilon\n",
        "    beta = 1/epsilon\n",
        "    for i in range(len(counts)):\n",
        "      counts[i] += np.random.laplace(0, beta, 1)\n",
        "    \n",
        "    # after adding noise we get labels with max counts\n",
        "    new_label = np.argmax(counts)\n",
        "    noisy_labels.append(new_label)\n",
        "  return np.array(noisy_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj1MUzf6bMx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f784b5ce-3230-4154-d57e-7e7caad10b50"
      },
      "source": [
        "labels_with_noise = add_noise(predicted_labels, epsilon=0.6)\n",
        "print(labels_with_noise)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 9 7 ... 5 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZlYVrfZkfF7",
        "colab_type": "text"
      },
      "source": [
        "## Step 5 Peform PATE Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj8M4w0GkY5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2439fc1f-4550-44ea-ac4d-b8cabb7bb7b5"
      },
      "source": [
        "# PATE analysis\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=predicted_labels.T, indices=labels_with_noise, noise_eps=0.1, delta=1e-5)\n",
        "print('Data dependent epsilon:', data_dep_eps)\n",
        "print('Data independent epsilon:', data_ind_eps)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data dependent epsilon: 291.51292546496586\n",
            "Data independent epsilon: 291.5129254649703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhh3PjvPkLgm",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Train **Student**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqgDXzDYkPYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "467aec07-dc3b-4462-d4f5-54a5b6b35431"
      },
      "source": [
        "# We have to create a new training dataloader for the student with the newly created \n",
        "# labels with noise. We have to replace the old labels with the new labels\n",
        "\n",
        "def new_student_data_loader(dataloader, noisy_labels, batch_size=64):\n",
        "  image_list = []\n",
        "  for image,_ in dataloader:\n",
        "    image_list.append(image)\n",
        "    \n",
        "  data = np.vstack(image_list)\n",
        "  new_dataset = list(zip(data, noisy_labels))\n",
        "  new_dataloader = DataLoader(new_dataset, batch_size, shuffle=False)\n",
        "\n",
        "  return new_dataloader\n",
        "\n",
        "# create a new dataloader for the public dataset\n",
        "# # using the new labels added global noise\n",
        "# def create_labeled_public_db(dataloader, labels, batch_size=64):\n",
        "#   image_list = []\n",
        "#   for image, _ in dataloader:\n",
        "#     image_list.append(image)\n",
        "#   data = np.vstack(image_list)\n",
        "#   new_dataloader = list(zip(data, labels))\n",
        "#   new_dataloader = torch.utils.data.DataLoader(new_dataloader, shuffle=False, batch_size=batch_size)\n",
        "#   return new_dataloader\n",
        "\n",
        "labeled_student_trainloader = new_student_data_loader(student_train_loader, labels_with_noise)\n",
        "len(labeled_student_trainloader)\n",
        "# student_trainloader = DataLoader(student_train_subset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "# student_validloader = DataLoader(student_valid_subset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "# public_loader_labeled = create_labeled_public_db(student_train_loader, labels_with_noise)\n",
        "# len(public_loader_labeled)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "110"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ30Hz87mWUc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "3fccba03-d7c1-454d-ecbb-1000cbee8b07"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Method to display Image for Tensor\n",
        "def imshow(image, ax=None, title=None, normalize=True):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    if normalize:\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "\n",
        "    ax.imshow(image)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.tick_params(axis='both', length=0)\n",
        "    ax.set_xticklabels('')\n",
        "    ax.set_yticklabels('')\n",
        "    return ax\n",
        "\n",
        "\n",
        "# Displaying Images and other info about the test set\n",
        "images, labels = next(iter(labeled_student_trainloader))\n",
        "# print(\" Image Size\",images.size())\n",
        "rand_idx = np.random.randint(len(images))\n",
        "# print(labels[rand_idx])\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(16,5), ncols=5)\n",
        "for ii in range(5):\n",
        "    ax = axes[ii]\n",
        "    # print(\" Image Size\",images[ii].size())\n",
        "    ax.set_title(labels[ii])\n",
        "    imshow(images[ii], ax=ax, normalize=True)\n",
        "\n",
        "       "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7blPzuFzZHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # train model using private and public datasets\n",
        "# my_model = Net()\n",
        "# criterion = nn.NLLLoss()\n",
        "# optimizer = optim.Adam(my_model.parameters(), lr=1e-3)\n",
        "# train(model=my_model, criterion=criterion, optimizer=optimizer, trainloader=private_loader, validloader=public_loader_labeled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lPctEDyVhEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "0ddb2915-53e0-4e16-d9e1-47cd53cfbc16"
      },
      "source": [
        "# Now we train the model\n",
        "# We use the newly labeled trainloader for training and use the validloader data to evaluate the performance of our model\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters() , lr=0.001)\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "student_model = train(model, criterion, optimizer,labeled_student_trainloader,student_valid_loader)\n",
        "\n",
        "# student_model = train(model, criterion, optimizer,student_trainloader, labeled_student_trainloader)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 0.471..  Valid Loss: 0.224..  Valid Accuracy: 0.940  \n",
            "Epoch: 2/10..  Training Loss: 0.447..  Valid Loss: 0.230..  Valid Accuracy: 0.939  \n",
            "Epoch: 3/10..  Training Loss: 0.438..  Valid Loss: 0.237..  Valid Accuracy: 0.931  \n",
            "Epoch: 4/10..  Training Loss: 0.432..  Valid Loss: 0.218..  Valid Accuracy: 0.940  \n",
            "Epoch: 5/10..  Training Loss: 0.417..  Valid Loss: 0.229..  Valid Accuracy: 0.936  \n",
            "Epoch: 6/10..  Training Loss: 0.409..  Valid Loss: 0.225..  Valid Accuracy: 0.938  \n",
            "Epoch: 7/10..  Training Loss: 0.401..  Valid Loss: 0.235..  Valid Accuracy: 0.933  \n",
            "Epoch: 8/10..  Training Loss: 0.383..  Valid Loss: 0.216..  Valid Accuracy: 0.941  \n",
            "Epoch: 9/10..  Training Loss: 0.399..  Valid Loss: 0.225..  Valid Accuracy: 0.938  \n",
            "Epoch: 10/10..  Training Loss: 0.406..  Valid Loss: 0.222..  Valid Accuracy: 0.940  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFK7rHBLklXw",
        "colab_type": "text"
      },
      "source": [
        "# TRAIN MNIST NORMALLY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKXwjzJhkU-k",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}